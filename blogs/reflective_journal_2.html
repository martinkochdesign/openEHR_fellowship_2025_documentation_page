  <h2>Reflective Journal 2</h2>
  
  <h3>Month 6... Already?</h3>

  <p>
    Time has flown by since our last fellowship work session (and the reflective journal). I was well aware that time would be a critical factor in developing my fellowship project, but *whoa!* the pace has truly surprised me.
  </p>
  
  <div style="text-align: center;">
    <img src="images/a_is_for_sabotage.png" alt="Who turned the clock?" width="180">
    <br>
    <small>Wer hat an der Uhr gedreht? (<i>Who turned the clock?</i>)</small>
  </div>
  
  <p>
    The past months was shaped not only by the holiday season, but also by <strong>EHRCON25</strong>, which had a notable impact on my project work. On the positive side, I had the opportunity to demonstrate the current state of my prototype to Heather and Keisha. Our conversations sparked a wealth of ideas and additional content for the project.
  </p>
  
  <p>
    On the other hand, though ultimately a good challenge, I was unexpectedly lucky enough to create and present a poster at the conference. This diverted some of my attention from the fellowship project itself, but the experience was rewarding. I particularly enjoyed this year's meeting, as I already knew many of the participants, especially fellow fellows and organizers.
  </p>
  
  <p>
    Progress on the application has continued steadily. In fact, I have already published an overview video of the current version. You can watch it here:
    <a href="https://martinkochdesign.github.io/openEHR_fellowship_2025_documentation_page//videos/application%20overview.webm" target="_blank">Application Overview Video</a>
  </p>

  <p>The next step is creating video tutorials, so the propspective users know every function of the application.</p>


  <h3>Month 5 fellowship session</h3>
  
  <p>I would like to extend my sincere thanks to all the presenters for the fellowship session on clinical modelling and community.</p>

  <p>This month's presentations offered a comprehensive look at the intersection of value-based healthcare and clinical modelling. I found the explanation of VBHC especially clear, and I fully agree with the importance of focusing on patient outcomes and the role of openEHR models in enabling meaningful data capture. 
  </p><p>
  The breakdown of archetype classes, referencing “An Ontology-based Model of Clinical Information,” was a highlight for me and will be useful in my own work. Heather's advice to resolve all data elements before starting modelling in openEHR tools resonated with my experience, as did the reminder to be mindful of copyright issues when reproducing information from archetypes, something I need to consider for my project. 
  </p><p>
  Silje's presentation was a timely reminder not to lose sight of the fundamentals, even as I delve deeper into the technical aspects of templates and archetypes. Seeing the "DIPS form designer" in action was new for me, and I appreciated the practical perspective. Overall, these sessions reinforced the value of a structured, thoughtful approach to clinical modelling and the importance of community learning and sharing.</p>

  <h3>Continuing Documentation</h3>

  <p>
      Over the past month, I have continued to document the development process. While documentation can be time-consuming, I have found it essential to first define what the final product should include before diving into the actual work. However, there are times when it is beneficial to explore and identify potential challenges early on.
  </p>
  
  <p>
      As a result, my current methodology is: <strong>explore &rarr; document &rarr; develop</strong>.
  </p>
  
  <p>
      To make the documentation more engaging, I introduced a character to break up the text visually and provide the reader with occasional relief. I must admit, taking a break to create a simple illustration is not only enjoyable but also adds a touch of fun to the process.
  </p>


  <div style="text-align: center;">
    <img src="images/a_is_for_family.png" alt="archetype illustrations" width="350">
    <br>
    <small>We're a happy family.</small>
  </div>
  
  <h3>AI, or Not AI: That Is the Question</h3>
  <p>
    As my fellowship project has progressed, one question keeps resurfacing: <strong>Why not just use Artificial Intelligence (AI)?</strong> It's 2025 - AI is everywhere, and it has proven itself capable of impressive semantic reasoning using Large Language Models (LLM).
  </p>

  <div style="text-align: center;">
    <img src="images/a_is_for_ai.png" alt="Using LLM" width="150">
    <br>
    <small>I, AI.</small>
  </div>
  
  <p>
    Setting aside ethical and environmental considerations for a moment, the real question becomes: <strong>What do we actually need AI to do?</strong> And, just as importantly, what information must we provide to AI so it can deliver meaningful results?
  </p>
  <p>
    The original scope of this project was to develop a tool for searching, recommending similar archetypes, planning, and visualizing archetypes to form data structures from a given list of data elements. For most of these functionalities, using AI would be like “shooting sparrows with cannons” - the benefits would be marginal compared to more deterministic algorithms.
  </p>
  <p>
    However, there is one area where AI could truly shine: <strong>suggesting whole data structures by intelligently assigning data elements to archetypes from CKM or other sources</strong>. This could give data modelers a valuable head start in creating those structures. The advantage of using AI here is its ability to consider all the semantic information embedded in archetypes and create meaningful links between data elements and archetypes.
  </p>
  <p>
    For this to work, the AI would need more than just a list of data elements. It would require additional context to evaluate the semantic connections to specific archetypes. This could be achieved by providing a description of the intended application for the data structure, as well as detailed descriptions for each data element.
  </p>
  <p>
    Here's how this could look in practice within the tool developed for this project:
  </p>
  <ul>
    <li>The user enters a list of data elements, supplying as much context as possible - both in the list and in the prompt for the AI (LLM).</li>
    <li>The AI analyzes the information from both the archetypes and the user's data elements, then suggests assignments of archetypes to each data element.</li>
    <li>This proposed data structure is then visualized in the tool, allowing the user to review and refine the AI's suggestions.</li>
    <li>Finally, the user can export the finalized plan and create openEHR templates in their editor of choice.</li>
  </ul>
  
  <div style="text-align: center;">
    <img src="images/using_ai.png" alt="Using LLM" width="700">
    <br>
    <small>Using LLM (AI) in context of this project.</small>
  </div>
   
  <p>
    It's also important to recognize that not every data modeller will approach this process in the same way. Some may prefer to construct data structures from the ground up, valuing the deep understanding and oversight that comes from doing this manually. Others might appreciate having a proposed structure generated by AI, using it as a starting point to accelerate their workflow and refine the results. By offering both options, the tool could accommodate different working styles and preferences, ensuring that AI serves as a helpful assistant.
  </p>
  <p>
    Whether this can be fully implemented during the course of this fellowship remains to be seen. Developing the data extraction and visualization tool as a one-person team - while also handling troubleshooting and documentation - is already a significant challenge.
  </p>
  <p>
    Nevertheless, this overview outlines a possible path for AI integration.
  </p>
  <p>
    <em>Special thanks to Lars and Heather for inspiring me to explore the potential of AI in this context.</em>
  </p>
